{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Segundo Entregable_Daniel_Quinones.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPAV/5izPph3Zesx0n3JGCn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/danquio194/github-slideshow/blob/master/Segundo_Entregable_Daniel_Quinones.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMlYhN5FNaQL"
      },
      "source": [
        "**Second Problems' Set -  Machine Learning Introduction**\n",
        "\n",
        "*by Daniel Felipe Quiñones Ordoñez*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pusjk0nDswfm"
      },
      "source": [
        "#Libraries for this notebook\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import math\n",
        "import random \n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eH920rmpNzAy"
      },
      "source": [
        "**Exercise 1.12**\n",
        "A friend comes to you with a learning problem. She says the target function $f$ is completly unknown, but she has $4000$ data points. She is willing to pay you to solve her problem and produce for her a $g$ which approximates $f$. What is the best that you can promise her among the following:\n",
        "\n",
        "\n",
        "\n",
        "1.   After learning you will provide her with a $g$ that you will guarantee approximates $f$ well out of sample.\n",
        "2.   After learning you will provide her with a $g$, and with high probability the $g$ which you produce will approximate $f$ well out of sample.\n",
        "3.   One of two things will happen:\n",
        "\n",
        "*   (i) You will produce a hypothesis $g$.\n",
        "*   (ii) You will declare that you failed\n",
        "\n",
        "If you do return a hypothesis $g$, then with high probability the g which you produce will approximate $f$ well out of sample.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBYPDZdSA-Z5"
      },
      "source": [
        "**Solution:**\n",
        "\n",
        "There is no guarantee that $g$ approximates $f$ well out of the sample. Even if we have a considerable huge amount of sample points. That could make us think in the (b) option, considering the probabilistic approach we accept for the feasability to learning out of the sample. However, still exists  the case that the learning not be succesful and the $E_{in}$ do not be close to zero neither close to $E_{out}$. Therefore the best answer we can give to our friend is the (c). Is more according to the reality: Though, as we seen in some of the exercises in this chapter, we always can give a $g$ hypothesis, there is no guarantee of succes in learning, and so we must have declare with shame our failed in the learning task. But, if we achieve in learn, with $4000$ sample points, and due to the Hoeffding inequality, the probability of $E_{in} \\approx E_{out}$ is less than a small number, being caotious in keep the $M$ number of hipothesys small as well. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XBxWF9CRTmX"
      },
      "source": [
        "**Problem 1.2**\n",
        "\n",
        "Consider the perceptron in two dimensions: $h(x) = sign(w^{T} x)$ where $w = [w_{0}, w_{1}, w_{2}]^{T}$ and $x = [1, x_{1},x_{2}]^{T}$. Tecnically $x$ has three coordinates, but we call this perceptron two-dimensional because the first coordinate is fixed at $1$. $ \\\\ $\n",
        "\n",
        "a). Show that the regions on the plane where $h(x) = +1$ and $h(x) = -1$ are separated by a line. If we express this line by the equation $x_{2} = ax_{1} + b$, what are the slope $a$ and the intercept $b$ in terms of $w_{0}, w_{1}, w_{2}$? $ \\\\ $\n",
        "\n",
        "b). Draw a picture for the cases $w = [1, 2, 3]^{T}$ and $w = -[1, 2, 3]^{T}$ \n",
        "\n",
        "**Solution:**\n",
        "\n",
        "a). An explicit formula for this perceptron is $w^{T}x = w_{0}x_{0} + w_{1}x_{1} + w_{2}x_{2} = w_{0} + w_{1}x_{1} + w_{2}x_{2}$\n",
        "\n",
        "For the region where $h(x) = +1$ we have that $w_{0} + w_{1}x_{1} + w_{2}x_{2} > 0$, and in the other hand, where $h(x) = -1$, $w_{0} + w_{1}x_{1} + w_{2}x_{2} < 0$. In this way, the line between those regions is the line $w_{0} + w_{1}x_{1} + w_{2}x_{2} = 0$. Rewriting the last equation \n",
        "\n",
        "$\n",
        "\\begin{equation}\n",
        "  x_{2} = \\frac{-w_{1}}{w_{2}}x_{1} - \\frac{w_{0}}{w_{2}}\n",
        "\\end{equation}\n",
        "$\n",
        "\n",
        "So, we can see that the scope $a = \\frac{-w_{1}}{w_{2}}$ and the intercept $b =- \\frac{w_{0}}{w_{2}}$\n",
        "\n",
        "b). For this part we plot the pictures in the following code box. As they are equivalent, their plots are the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NeMlvnqZsrjB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "653e442c-e2b0-4582-ebc9-ee1f8672f7b9"
      },
      "source": [
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "line1 = -(-2/-3)*x - 1/3\n",
        "line2 = -(-2/-3)*x - (-1/-3)\n",
        "\n",
        "plt.figure(1)\n",
        "#plt.figure(1).suptitle('f(x)−cos(x), x_0 = 0.5')\n",
        "plt.subplot(121)\n",
        "plt.plot(x, line1, 'ro')\n",
        "plt.title('Line 1') \n",
        "\n",
        "plt.subplot(122)\n",
        "plt.plot(x, line1, 'bo')\n",
        "plt.title('Line 2')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Line 2')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSSw_wJtT-jI"
      },
      "source": [
        "**Problem 1.4** In exercise 1.4, we use an artificial data set to study the perceptron learning algorithm. This problem leads you to explore the algorithm further with data sets od different sizes and dimensions:\n",
        "\n",
        "(a). Generate a linearly separate data set of size 20 as indicated in Exercise 1.4. Plot the examples $\\{ (x_{n}, y_{n}) \\}$ as well as the target function $f$ on a plane. Be sure to mark the examples from different classes differently and add labels to the axes of the plot.\n",
        "\n",
        "(b). Run the perceptron learning algorithm on the data set above. Report the number of updates that the algorithm takes before converging. Plot the examples $\\{ (x_{n}, y_{n}) \\}$, the target function $f$, and the final hypothesis $g$ in the same figure. Comment on whether $f$ is close to $g$.\n",
        "\n",
        "(c). Repeat everything in (b) with another randomly generated data set of size 20. Compare your results with (b).\n",
        "\n",
        "(d). Repeat everything in (b) with another randomly generated data set of size 100. Compare your results with (b).\n",
        "\n",
        "(e). Repeat everything in (b) with another randomly generated data set of size 1000. Compare your results with (b).\n",
        "\n",
        "(f). Modify the algorithm such that it takes $x_{n} \\in \\mathbb{R}^{10}$ instead of $\\mathbb{R}^{2}$. Randmoly generate a linearly separable data set of size $1000$ with ${x_{n}}\\in \\mathbb{R}^{10}$ and feed the data set to the algorithm. How many updates does the algorithm take to converge?\n",
        "\n",
        "(h). Summarize your conclusions with respect to accuracy andd running time as a function of $N$ and $d$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fdy0GjWKDSvw"
      },
      "source": [
        "**Solution:**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kOHQj1bRpHq"
      },
      "source": [
        "#Picking a random target function f\n",
        "x = np.arange(-5.0, 5.0, 0.1)\n",
        "m = random.uniform(-10, 10)\n",
        "b = random.uniform(-10, 10)\n",
        "\n",
        "def random_f(x, b):\n",
        "  return  m*x + b\n",
        "\n",
        "target_f = random_f(x, b)\n",
        "#Setting the random samples points\n",
        "\n",
        "def randomSet(n, domain):\n",
        "  x_n = []\n",
        "  y_n_red = []\n",
        "  y_n_green = []\n",
        "  for i in range(n):\n",
        "    point = random.uniform(-10, 10)\n",
        "    x_n.append(point)\n",
        "    if (point < b):\n",
        "      y_n_red.append(random.uniform(-10, 10)*0.5 + 1)\n",
        "    else:\n",
        "      y_n_green.append(random.uniform(-10, 10)*(-0.5) - 1)\n",
        "    i += 1\n",
        "  return x_n, y_n_green, y_n_red\n",
        "\n",
        "#samples = (x_n, random_f(m, x_n, b))\n",
        "axes = randomSet(20, x)\n",
        "\n",
        "#print(samples)\n",
        "plt.subplot(131)\n",
        "plt.plot(x, target_f, 'ko')\n",
        "\n",
        "plt.subplot(132)\n",
        "plt.plot(axes[0], axes[1], 'ro')\n",
        "\n",
        "plt.subplot(133)\n",
        "plt.plot(axes[0], axes[2], 'go' )\n",
        "#, np.asarray(randomSet(20, x)[0]), np.asarray(randomSet(20, x)[1]), 'ro')\n",
        "\n",
        "#, np.asarray(randomSet(20, x)[0]), np.asarray(randomSet(20, x)[2]), 'go' )\n",
        "\n",
        "print(x_n)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwpR_kjL5yYp"
      },
      "source": [
        "**Problem 1.6** Consider a sample of 10 marbles drawn independtly from a bin that holds red and green marbles. The probability of a red marble is $\\mu$. For $\\mu = 0.05, \\mu = 0.5,$ and $\\mu = 0.8$, compute the probability of getting no red marbles ($\\nu = 0$) in the following cases:\n",
        "\n",
        "(a). We draw only one such a sample. Compute the probability that  $\\nu = 0$.\n",
        "\n",
        "(b). We draw $1000$ independent samples. Compute the probability that (at least) one of the samples has $\\nu = 0$.\n",
        "\n",
        "(c). Repeat (b) for $1000000$ independent samples."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucyMoj5a1d33"
      },
      "source": [
        "**Solution:** Notice that $1 - \\mu$ is the probability that the event \"no red\" occurs. For the independence of the drawing a bin, we get that $Pr(\\nu = 0) = (1 - \\mu)^{10}$ for each sample of 10 bins. \n",
        "Therefore we get as follows:\n",
        "\n",
        "(a)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EVbNmZq-Wms1",
        "outputId": "62a1b5ad-ced9-4b17-db84-120aabe81881"
      },
      "source": [
        "#Draw only one sample:\n",
        "\n",
        "print(\"Probability of v=0 with mu = 0.05 =\" , 0.95**10)\n",
        "print(\"Probability of v=0 with mu = 0.5 =\", 0.5**10)\n",
        "print(\"Probability of v=0 with mu = 0.8 =\", 0.2**10)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Probability of v=0 with mu = 0.05 = 0.5987369392383787\n",
            "Probability of v=0 with mu = 0.5 = 0.0009765625\n",
            "Probability of v=0 with mu = 0.8 = 1.0240000000000006e-07\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v896qwzk3LlI"
      },
      "source": [
        "(b). For this consider the event when none sample is $\\nu = 0$. That is when $\\nu > 0$ or, \n",
        "\n",
        "\\begin{equation}\n",
        "  Pr(\\nu > 0) = 1 - Pr(\\nu = 0) = 1 - (1 - \\mu)^{10}\n",
        "\\end{equation}\n",
        "\n",
        "As we have independence when we choose the marbles' samples, then for $1000$ drwas\n",
        "\n",
        "\\begin{align*}\n",
        "  \\prod_{i = 1}^{1000} Pr(\\nu_{i} > 0) &= \\prod_{i = 1}^{1000} 1 - Pr(\\nu_{i} = 0) \\\\ \n",
        "                                  &=  \\prod_{i = 1}^{1000} 1 - (1 - \\mu)^{10} \\\\\n",
        "                                  &= (1 - (1 - \\mu)^{10})^{1000}\n",
        "\\end{align*}\n",
        "\n",
        "Taking the complement of the last expression we get the desire probability. We show the compute when we make $1000$ draws samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DenAT4FL2wrk",
        "outputId": "4f94162c-e262-42d6-e14c-c46e05602f9b"
      },
      "source": [
        "#We define a function with the compute made before when we make 1000 draws\n",
        "#to pick the samples' bins\n",
        "def binExperiment(mu, n):\n",
        "  return 1 - ((1 - (1-mu)**10)**n)\n",
        "\n",
        "print(\"When mu = 0.05, the probability is \", binExperiment(0.05, 1000))\n",
        "print(\"When mu = 0.5, the probability is \", binExperiment(0.5, 1000))\n",
        "print(\"When mu = 0.8, the probability is \", binExperiment(0.8, 1000))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When mu = 0.05, the probability is  1.0\n",
            "When mu = 0.5, the probability is  0.623576201943276\n",
            "When mu = 0.8, the probability is  0.00010239476257623004\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qAyL0LhO66eV"
      },
      "source": [
        "(c). We repeat the last case, but with $1000000$ independent samples."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ezTXU8nR65rp",
        "outputId": "3ed5edbb-c4ac-439e-ec95-b0fe9f9e5fb6"
      },
      "source": [
        "#We define a function with the compute made before when we make 1000000 draws\n",
        "#to pick the samples' bins\n",
        "def binExperiment(mu, n):\n",
        "  return 1 - ((1 - (1-mu)**10)**n)\n",
        "\n",
        "print(\"When mu = 0.05, the probability is \", binExperiment(0.05, 10000000))\n",
        "print(\"When mu = 0.5, the probability is \", binExperiment(0.5, 1000000))\n",
        "print(\"When mu = 0.8, the probability is \", binExperiment(0.8, 1000000))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When mu = 0.05, the probability is  1.0\n",
            "When mu = 0.5, the probability is  1.0\n",
            "When mu = 0.8, the probability is  0.09733159268316072\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqXsSpoQWv_m"
      },
      "source": [
        "**Problem 1.9** In this problem, we derive a form of the law of larger numbers that has an exponential bound, called the *Chernoff bound*. We focus on the simple case of flipping a fair coin, and use an approach similar to problem 1.8.\n",
        "\n",
        "(a). Let $t$ be a finite random variable, $\\alpha$ be a positive constant, and $s$ be a positive parameter. If $T(s) = \\mathbb{E}_{t}(e^{st})$ prove that \n",
        "\n",
        "\\begin{equation}\n",
        "  P[t \\geq \\alpha] \\leq e^{-s \\alpha}T(s)\n",
        "\\end{equation}\n",
        "\n",
        "(b). Let $u_{1}, u_{2}, ..., u_{N}$ be iid random variables, and let $u = \\sum_{i=1}^{N} u_{n}$. If $U(s) = \\mathbb{E}_{u_{n}} (e^{s u_{n}})$ for any n, prove that \n",
        "\n",
        "\\begin{equation}\n",
        "  P[u \\geq \\alpha] \\leq (e^{-s \\alpha}U(s))^{N}\n",
        "\\end{equation}\n",
        "\n",
        "(c). Suppose $P[u_{n} = 0] = P[u_{n} = 1] = \\frac{1}{2}$ (fair coin). Evaluate $U(s)$ as a function of $s$, and minimize $e^{-s \\alpha} U(s)$ with respect to $s$ for fixed $\\alpha$, $0 < \\alpha < 1$.\n",
        "\n",
        "(d) Conclude in (c) that, for $0 < \\frac{\\epsilon}{2} < 1$,\n",
        "\n",
        "\\begin{equation}\n",
        "  P[u \\geq \\mathbb{E}(u) + \\epsilon] \\leq 2^{-\\beta N}\n",
        "\\end{equation}\n",
        "\n",
        "where $\\beta = 1 + (\\frac{1}{2} + \\epsilon)log_{2}(\\frac{1}{2} + \\epsilon) + (\\frac{1}{2} - \\epsilon)log_{2}(\\frac{1}{2} - \\epsilon)$ and $\\mathbb{E}(u) = \\frac{1}{2}$. Show that $\\beta > 0$, hence the bound is exponentially decreasing in $N$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BgaB5Ycbb2u"
      },
      "source": [
        "**Solution:**\n",
        "\n",
        "(a). We get that for Markov's inequality $P[X \\geq \\alpha] \\leq \\frac{\\mathbb{E}(X)}{a}$ with $a \\in \\mathbb{R}$. Thus we can apply this inequality to \n",
        "\n",
        "\\begin{equation}\n",
        "  P[t \\geq \\alpha] = P[e^{st} \\geq e^{s \\alpha}] \\leq \\frac{\\mathbb{E}(e^{st})}{e^{s \\alpha}}\n",
        "\\end{equation}\n",
        "\n",
        "that is what we want to prove.\n",
        "\n",
        "(b). Using the last assertion and applying it to $P[Nu \\geq N \\alpha]$, with an analogous argument we get that \n",
        "\n",
        "\\begin{align*}\n",
        "  P[Nu \\geq N \\alpha] &= P[e^{sNu} \\geq e^{sN\\alpha}] \\\\\n",
        "  P[u \\geq  \\alpha]   & \\leq e^{-sN\\alpha} \\mathbb{E}(e^{sNu}) \\\\\n",
        "                      & \\leq e^{-sN\\alpha} \\mathbb{E}(e^{s \\sum_{n=1}^{N} u_{n}}) \\\\\n",
        "                      & \\leq e^{-sN\\alpha} \\mathbb{E}(\\prod_{n=1}^{N} e^{s u_{n}}) \\\\\n",
        "                      & \\leq e^{-sN\\alpha} \\prod_{n=1}^{N} \\mathbb{E} (e^{s u_{n}}) \\\\\n",
        "\\end{align*}\n",
        "\n",
        "As for $n = 1, ..., N$, $u_{n}$ are iid, we get that \n",
        "\n",
        "\\begin{align*}\n",
        "  P[u \\geq  \\alpha] & \\leq e^{-sN\\alpha} [\\mathbb{E} (e^{s u_{n}})]^{N} \\\\\n",
        "                    & \\leq [e^{-s\\alpha} \\mathbb{E} (e^{s u_{n}})] ^{N}\n",
        "\\end{align*}\n",
        "\n",
        "finishing the prove.\n",
        "\n",
        "(c). We get that for definition of the expectancie and its properties, \n",
        "\n",
        "$U(s) = \\mathbb{E}_{u_{n}}(e^{su_{n}}) = P[u_{n} = 0]e^{0} + P[u_{n} = 1] = \\frac{1}{2}(1 + e^{s})$.\n",
        "\n",
        "To minimize the last expression with respect to s, lets derivate \n",
        "\n",
        "\\begin{equation}\n",
        "  G(s) = e^{-s \\alpha} \\frac{1}{2} (1 + e^{s}) = \\frac{1}{2}(e^{-s \\alpha} + e^{s(1 - \\alpha)})\n",
        "\\end{equation}\n",
        "\n",
        "with respect to s and find its minimum point.\n",
        "\n",
        "\\begin{equation}\n",
        "  \\frac{dG(s)}{ds} = \\frac{-\\alpha}{2} e^{-s \\alpha} + \\frac{1 -\\alpha}{2} e^{s(1 - \\alpha)} = 0\n",
        "\\end{equation}\n",
        "\n",
        "Then,\n",
        "\n",
        "\\begin{align*}\n",
        "  (1 - \\alpha) e^{s(1 - \\alpha)} &= \\alpha e^{-s \\alpha} \\\\\n",
        "  e^{s(1 - \\alpha)} e^{-s \\alpha} &= \\frac{\\alpha}{1 - \\alpha} \\\\\n",
        "  e^{s} &= \\frac{\\alpha}{1 - \\alpha} \\\\\n",
        "  s &= ln(\\frac{\\alpha}{1 - \\alpha}) > 0\n",
        "\\end{align*}\n",
        "\n",
        "since $0 < \\alpha < 1$. Therefore this value is a minimum for $G(s)$.\n"
      ]
    }
  ]
}